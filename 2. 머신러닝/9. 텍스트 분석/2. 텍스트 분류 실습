{"cells":[{"cell_type":"markdown","metadata":{"id":"yN3QcB7xFSCb"},"source":["#1. 데이터 읽어 오기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1069,"status":"ok","timestamp":1644459979758,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"LwtzQtphFCf7","outputId":"47911ac5-0471-4927-8645-a258539f0b1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"]}],"source":["from sklearn.datasets import fetch_20newsgroups\n","\n","news_data = fetch_20newsgroups(subset='all', random_state=62)\n","\n","print(news_data.keys())"]},{"cell_type":"markdown","metadata":{"id":"uwI0GZX1FjGj"},"source":["#2. 데이터 전처리"]},{"cell_type":"markdown","metadata":{"id":"1rKxXpkfFkvK"},"source":["##2.1 데이터 확인"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1644459983451,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"e95FZxA0Fegj","outputId":"85d0ca87-f43f-463a-f525-580e8d89b43e"},"outputs":[{"name":"stdout","output_type":"stream","text":["target 클래스의 값과 분포도\n"," 0     799\n","1     973\n","2     985\n","3     982\n","4     963\n","5     988\n","6     975\n","7     990\n","8     996\n","9     994\n","10    999\n","11    991\n","12    984\n","13    990\n","14    987\n","15    997\n","16    910\n","17    940\n","18    775\n","19    628\n","dtype: int64\n","target 클래스의 이름들\n"," ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"]}],"source":["import pandas as pd\n","\n","# 타겟별 개수\n","print('target 클래스의 값과 분포도\\n',pd.Series(news_data.target).value_counts().sort_index())\n","# 타겟 명\n","print('target 클래스의 이름들\\n',news_data.target_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1644459991518,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"P5-nmoigGuka","outputId":"018da980-0f58-4715-e7e0-463c083e209e"},"outputs":[{"name":"stdout","output_type":"stream","text":[".. _20newsgroups_dataset:\n","\n","The 20 newsgroups text dataset\n","------------------------------\n","\n","The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n","20 topics split in two subsets: one for training (or development)\n","and the other one for testing (or for performance evaluation). The split\n","between the train and test set is based upon a messages posted before\n","and after a specific date.\n","\n","This module contains two loaders. The first one,\n",":func:`sklearn.datasets.fetch_20newsgroups`,\n","returns a list of the raw texts that can be fed to text feature\n","extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n","with custom parameters so as to extract feature vectors.\n","The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n","returns ready-to-use features, i.e., it is not necessary to use a feature\n","extractor.\n","\n","**Data Set Characteristics:**\n","\n","    =================   ==========\n","    Classes                     20\n","    Samples total            18846\n","    Dimensionality               1\n","    Features                  text\n","    =================   ==========\n","\n","Usage\n","~~~~~\n","\n","The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n","fetching / caching functions that downloads the data archive from\n","the original `20 newsgroups website`_, extracts the archive contents\n","in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",":func:`sklearn.datasets.load_files` on either the training or\n","testing set folder, or both of them::\n","\n","  \u003e\u003e\u003e from sklearn.datasets import fetch_20newsgroups\n","  \u003e\u003e\u003e newsgroups_train = fetch_20newsgroups(subset='train')\n","\n","  \u003e\u003e\u003e from pprint import pprint\n","  \u003e\u003e\u003e pprint(list(newsgroups_train.target_names))\n","  ['alt.atheism',\n","   'comp.graphics',\n","   'comp.os.ms-windows.misc',\n","   'comp.sys.ibm.pc.hardware',\n","   'comp.sys.mac.hardware',\n","   'comp.windows.x',\n","   'misc.forsale',\n","   'rec.autos',\n","   'rec.motorcycles',\n","   'rec.sport.baseball',\n","   'rec.sport.hockey',\n","   'sci.crypt',\n","   'sci.electronics',\n","   'sci.med',\n","   'sci.space',\n","   'soc.religion.christian',\n","   'talk.politics.guns',\n","   'talk.politics.mideast',\n","   'talk.politics.misc',\n","   'talk.religion.misc']\n","\n","The real data lies in the ``filenames`` and ``target`` attributes. The target\n","attribute is the integer index of the category::\n","\n","  \u003e\u003e\u003e newsgroups_train.filenames.shape\n","  (11314,)\n","  \u003e\u003e\u003e newsgroups_train.target.shape\n","  (11314,)\n","  \u003e\u003e\u003e newsgroups_train.target[:10]\n","  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n","\n","It is possible to load only a sub-selection of the categories by passing the\n","list of the categories to load to the\n",":func:`sklearn.datasets.fetch_20newsgroups` function::\n","\n","  \u003e\u003e\u003e cats = ['alt.atheism', 'sci.space']\n","  \u003e\u003e\u003e newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n","\n","  \u003e\u003e\u003e list(newsgroups_train.target_names)\n","  ['alt.atheism', 'sci.space']\n","  \u003e\u003e\u003e newsgroups_train.filenames.shape\n","  (1073,)\n","  \u003e\u003e\u003e newsgroups_train.target.shape\n","  (1073,)\n","  \u003e\u003e\u003e newsgroups_train.target[:10]\n","  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n","\n","Converting text to vectors\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","In order to feed predictive or clustering models with the text data,\n","one first need to turn the text into vectors of numerical values suitable\n","for statistical analysis. This can be achieved with the utilities of the\n","``sklearn.feature_extraction.text`` as demonstrated in the following\n","example that extract `TF-IDF`_ vectors of unigram tokens\n","from a subset of 20news::\n","\n","  \u003e\u003e\u003e from sklearn.feature_extraction.text import TfidfVectorizer\n","  \u003e\u003e\u003e categories = ['alt.atheism', 'talk.religion.misc',\n","  ...               'comp.graphics', 'sci.space']\n","  \u003e\u003e\u003e newsgroups_train = fetch_20newsgroups(subset='train',\n","  ...                                       categories=categories)\n","  \u003e\u003e\u003e vectorizer = TfidfVectorizer()\n","  \u003e\u003e\u003e vectors = vectorizer.fit_transform(newsgroups_train.data)\n","  \u003e\u003e\u003e vectors.shape\n","  (2034, 34118)\n","\n","The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n","components by sample in a more than 30000-dimensional space\n","(less than .5% non-zero features)::\n","\n","  \u003e\u003e\u003e vectors.nnz / float(vectors.shape[0])\n","  159.01327...\n","\n",":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which\n","returns ready-to-use token counts features instead of file names.\n","\n",".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n","\n","\n","Filtering text for more realistic training\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","It is easy for a classifier to overfit on particular things that appear in the\n","20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n","high F-scores, but their results would not generalize to other documents that\n","aren't from this window of time.\n","\n","For example, let's look at the results of a multinomial Naive Bayes classifier,\n","which is fast to train and achieves a decent F-score::\n","\n","  \u003e\u003e\u003e from sklearn.naive_bayes import MultinomialNB\n","  \u003e\u003e\u003e from sklearn import metrics\n","  \u003e\u003e\u003e newsgroups_test = fetch_20newsgroups(subset='test',\n","  ...                                      categories=categories)\n","  \u003e\u003e\u003e vectors_test = vectorizer.transform(newsgroups_test.data)\n","  \u003e\u003e\u003e clf = MultinomialNB(alpha=.01)\n","  \u003e\u003e\u003e clf.fit(vectors, newsgroups_train.target)\n","  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n","\n","  \u003e\u003e\u003e pred = clf.predict(vectors_test)\n","  \u003e\u003e\u003e metrics.f1_score(newsgroups_test.target, pred, average='macro')\n","  0.88213...\n","\n","(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n","the training and test data, instead of segmenting by time, and in that case\n","multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n","yet of what's going on inside this classifier?)\n","\n","Let's take a look at what the most informative features are:\n","\n","  \u003e\u003e\u003e import numpy as np\n","  \u003e\u003e\u003e def show_top10(classifier, vectorizer, categories):\n","  ...     feature_names = vectorizer.get_feature_names_out()\n","  ...     for i, category in enumerate(categories):\n","  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n","  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n","  ...\n","  \u003e\u003e\u003e show_top10(clf, vectorizer, newsgroups_train.target_names)\n","  alt.atheism: edu it and in you that is of to the\n","  comp.graphics: edu in graphics it is for and of to the\n","  sci.space: edu it that is in and space to of the\n","  talk.religion.misc: not it you in is that and to of the\n","\n","\n","You can now see many things that these features have overfit to:\n","\n","- Almost every group is distinguished by whether headers such as\n","  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n","- Another significant feature involves whether the sender is affiliated with\n","  a university, as indicated either by their headers or their signature.\n","- The word \"article\" is a significant feature, based on how often people quote\n","  previous posts like this: \"In article [article ID], [name] \u003c[e-mail address]\u003e\n","  wrote:\"\n","- Other features match the names and e-mail addresses of particular people who\n","  were posting at the time.\n","\n","With such an abundance of clues that distinguish newsgroups, the classifiers\n","barely have to identify topics from text at all, and they all perform at the\n","same high level.\n","\n","For this reason, the functions that load 20 Newsgroups data provide a\n","parameter called **remove**, telling it what kinds of information to strip out\n","of each file. **remove** should be a tuple containing any subset of\n","``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n","blocks, and quotation blocks respectively.\n","\n","  \u003e\u003e\u003e newsgroups_test = fetch_20newsgroups(subset='test',\n","  ...                                      remove=('headers', 'footers', 'quotes'),\n","  ...                                      categories=categories)\n","  \u003e\u003e\u003e vectors_test = vectorizer.transform(newsgroups_test.data)\n","  \u003e\u003e\u003e pred = clf.predict(vectors_test)\n","  \u003e\u003e\u003e metrics.f1_score(pred, newsgroups_test.target, average='macro')\n","  0.77310...\n","\n","This classifier lost over a lot of its F-score, just because we removed\n","metadata that has little to do with topic classification.\n","It loses even more if we also strip this metadata from the training data:\n","\n","  \u003e\u003e\u003e newsgroups_train = fetch_20newsgroups(subset='train',\n","  ...                                       remove=('headers', 'footers', 'quotes'),\n","  ...                                       categories=categories)\n","  \u003e\u003e\u003e vectors = vectorizer.fit_transform(newsgroups_train.data)\n","  \u003e\u003e\u003e clf = MultinomialNB(alpha=.01)\n","  \u003e\u003e\u003e clf.fit(vectors, newsgroups_train.target)\n","  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n","\n","  \u003e\u003e\u003e vectors_test = vectorizer.transform(newsgroups_test.data)\n","  \u003e\u003e\u003e pred = clf.predict(vectors_test)\n","  \u003e\u003e\u003e metrics.f1_score(newsgroups_test.target, pred, average='macro')\n","  0.76995...\n","\n","Some other classifiers cope better with this harder version of the task. Try\n","running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n","the ``--filter`` option to compare the results.\n","\n",".. topic:: Data Considerations\n","\n","  The Cleveland Indians is a major league baseball team based in Cleveland,\n","  Ohio, USA. In December 2020, it was reported that \"After several months of\n","  discussion sparked by the death of George Floyd and a national reckoning over\n","  race and colonialism, the Cleveland Indians have decided to change their\n","  name.\" Team owner Paul Dolan \"did make it clear that the team will not make\n","  its informal nickname -- the Tribe -- its new team name.\" \"It’s not going to\n","  be a half-step away from the Indians,\" Dolan said.\"We will not have a Native\n","  American-themed name.\"\n","\n","  https://www.mlb.com/news/cleveland-indians-team-name-change\n","\n",".. topic:: Recommendation\n","\n","  - When evaluating text classifiers on the 20 Newsgroups data, you\n","    should strip newsgroup-related metadata. In scikit-learn, you can do this\n","    by setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n","    lower because it is more realistic.\n","  - This text dataset contains data which may be inappropriate for certain NLP\n","    applications. An example is listed in the \"Data Considerations\" section\n","    above. The challenge with using current text datasets in NLP for tasks such\n","    as sentence completion, clustering, and other applications is that text\n","    that is culturally biased and inflammatory will propagate biases. This\n","    should be taken into consideration when using the dataset, reviewing the\n","    output, and the bias should be documented.\n","\n",".. topic:: Examples\n","\n","   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n","\n","   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n","\n"]}],"source":["# 간단하게 사용법 확인, 데이터 제거 확인, remove=('headers', 'footers', 'quotes')\n","print(news_data.DESCR)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":460,"status":"ok","timestamp":1644459996840,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"EaEmwiT9Fxbi","outputId":"5adafc9a-7f7f-4435-eac7-a554f063a46e"},"outputs":[{"name":"stdout","output_type":"stream","text":["From: barmar@think.com (Barry Margolin)\n","Subject: Re: XV 3.00 has escaped!\n","Organization: Thinking Machines Corporation, Cambridge MA, USA\n","Lines: 25\n","NNTP-Posting-Host: telecaster.think.com\n","\n","In article \u003c1993Apr29.102341.13820@comp.lancs.ac.uk\u003e julian@comp.lancs.ac.uk (Julian G. Self) writes:\n","\u003eWasn't the shareware fee a \"suggestion\" by John?\n","\n","It's a request to personal users; it's a requirement for commercial,\n","government, and institutional users.\n","\n","Someone else asked whether the authors of the JPEG and TIFF software had\n","given permission to incorporate their code into a commercial product.  I\n","found the following in jpeg/README:\n","\n","    We specifically permit and encourage the use of this software as the\n","    basis of commercial products, provided that all warranty or liability\n","    claims are assumed by the product vendor.\n","\n","and the following in tiff/Copyright:\n","\n","    Permission to use, copy, modify, distribute, and sell this software and\n","    its documentation for any purpose is hereby granted without fee, ...\n","\n","Looks like he's OK on that account.\n","-- \n","Barry Margolin\n","System Manager, Thinking Machines Corp.\n","\n","barmar@think.com          {uunet,harvard}!think!barmar\n","\n"]}],"source":["print(news_data.data[0])"]},{"cell_type":"markdown","metadata":{"id":"3eXQEOlTH8YB"},"source":["##2.2 데이터 분류"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22616,"status":"ok","timestamp":1644541153573,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"artSZldeGPd6","outputId":"402d8fa3-2989-47d7-cec1-ed99e1f7e3b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003cclass 'list'\u003e\n","학습 데이터 크기 11314 , 테스트 데이터 크기 7532\n"]}],"source":["from sklearn.datasets import fetch_20newsgroups\n","\n","#subset='train'으로 학습용(Train) 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n","train_news= fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=62)\n","X_train = train_news.data\n","y_train = train_news.target\n","print(type(X_train))\n","\n","#subset='test'으로 테스트(Test) 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n","test_news= fetch_20newsgroups(subset='test',remove=('headers', 'footers','quotes'),random_state=62)\n","X_test = test_news.data\n","y_test = test_news.target\n","print('학습 데이터 크기 {0} , 테스트 데이터 크기 {1}'.format(len(train_news.data) , len(test_news.data)))"]},{"cell_type":"markdown","metadata":{"id":"X5acSJspID4J"},"source":["##2.3 데이터 벡터화"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5758,"status":"ok","timestamp":1644460022610,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"z-zsolrwIEIi","outputId":"a88ca249-311f-45e6-c505-8cbca3354e88"},"outputs":[{"name":"stdout","output_type":"stream","text":["학습 데이터 Text의 CountVectorizer Shape: (11314, 101631)\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","#Count Vectorization으로 feature extraction 변환 수행\n","cnt_vect = CountVectorizer()\n","cnt_vect.fit(X_train)\n","X_train_cnt_vect = cnt_vect.transform(X_train)\n","\n","#학습 데이터로 fit( )된 CountVectorizer를 이용하여 테스트 데이터를 feature extraction 변환 수행\n","X_test_cnt_vect = cnt_vect.transform(X_test)\n","\n","print('학습 데이터 Text의 CountVectorizer Shape:',X_train_cnt_vect.shape)"]},{"cell_type":"markdown","metadata":{"id":"RzHlv4caIcuB"},"source":["#3. 모델 생성"]},{"cell_type":"markdown","metadata":{"id":"0XDhpvZKdQX1"},"source":["##3.1 모델 만들기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":278442,"status":"ok","timestamp":1644460307540,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"MHZT36dWIdA5","outputId":"16e0ab1c-736f-49c5-a89a-cfe9caaca598"},"outputs":[{"data":{"text/plain":["LogisticRegression(max_iter=1000)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# 5분정도 소요\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","#LogisticRegression을 이용하여 학습/예측/평가 수행\n","# ConvergenceWarning: lbfgs failed to converge 경고가 뜰 경우 max_iter=1000 추가 후 수치 조절\n","lr_clf = LogisticRegression(max_iter=1000)\n","lr_clf.fit(X_train_cnt_vect , y_train)"]},{"cell_type":"markdown","metadata":{"id":"aqYoZzGVdTZN"},"source":["##3.2 예측"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":633,"status":"ok","timestamp":1644460327493,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"9AeYlUX3c3qG","outputId":"4b1aad13-31c3-4546-fcd9-2311a5d96615"},"outputs":[{"name":"stdout","output_type":"stream","text":["CountVectorized Logistic Regression의 예측 정확도는 0.597\n"]}],"source":["pred = lr_clf.predict(X_test_cnt_vect)\n","print('CountVectorized Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))"]},{"cell_type":"markdown","metadata":{"id":"rMmO8osGIpsB"},"source":["#4. TF-IDF 기반 모델 생성(Quiz)"]},{"cell_type":"markdown","metadata":{"id":"lhXJY3aLdkZO"},"source":["##4.1 벡터화"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTWze6UKItti"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","#TF-IDF Vectorization 적용하여 학습 데이터 세트와 테스트 데이터 세트 변환\n","tfidf_vect = TfidfVectorizer()\n","tfidf_vect.fit(X_train)\n","X_train_tfidf_vect = tfidf_vect.transform(X_train)\n","X_test_tfidf_vect = tfidf_vect.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"n7NaBVQndsiF"},"source":["##4.2 모델 만들기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45211,"status":"ok","timestamp":1644460386131,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"ZhcEBESrdi30","outputId":"5a351111-c26e-4070-9b08-d51edea3b34f"},"outputs":[{"data":{"text/plain":["LogisticRegression(max_iter=1000)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["#1분 소요\n","lr_clf = LogisticRegression(max_iter=1000)\n","lr_clf.fit(X_train_tfidf_vect , y_train)"]},{"cell_type":"markdown","metadata":{"id":"2ZCpeG_Pdv1l"},"source":["##4.3 예측"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":714,"status":"ok","timestamp":1644460409516,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"jPwg0J0Mdx79","outputId":"142ce578-7f91-468f-e065-f510aa7d5244"},"outputs":[{"name":"stdout","output_type":"stream","text":["TF-IDF Logistic Regression의 예측 정확도는 0.674\n"]}],"source":["pred = lr_clf.predict(X_test_tfidf_vect)\n","print('TF-IDF Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"]},{"cell_type":"markdown","metadata":{"id":"Rs8UIBNgQe_9"},"source":["#5. 전처리 후 모델 생성"]},{"cell_type":"markdown","metadata":{"id":"1dOrL6Txer4l"},"source":["##5.1 데이터 벡터화"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQeXAr5wQj90"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","#stop words 필터링을 추가하고 ngram을 기본(1,1)에서 (1,2)로 변경하여 Feature Vectorization 적용\n","tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300 )\n","tfidf_vect.fit(X_train)\n","X_train_tfidf_vect = tfidf_vect.transform(X_train)\n","X_test_tfidf_vect = tfidf_vect.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"NqmzFRy0e3X8"},"source":["##5.2 모델 만들기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245157,"status":"ok","timestamp":1644460850497,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"kbwQEyO-exc4","outputId":"9394fbfd-e357-4f7e-b1ce-47e41cb5a97b"},"outputs":[{"data":{"text/plain":["LogisticRegression(max_iter=1000)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# 4분 소요\n","from sklearn.linear_model import LogisticRegression\n","\n","lr_clf = LogisticRegression(max_iter=1000)\n","lr_clf.fit(X_train_tfidf_vect , y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":469,"status":"ok","timestamp":1644461277576,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"DZsZxCKpK72O","outputId":"3e7f0a17-4845-48ef-fd7d-301a1e41905a"},"outputs":[{"name":"stdout","output_type":"stream","text":["TF-IDF Vectorized Logistic Regression의 예측 정확도는 0.692\n"]}],"source":["from sklearn.metrics import accuracy_score\n","\n","pred = lr_clf.predict(X_test_tfidf_vect)\n","print('TF-IDF Vectorized Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"]},{"cell_type":"markdown","metadata":{"id":"xWqWLvE8QxeE"},"source":["#6. 하이퍼 파라미터"]},{"cell_type":"markdown","metadata":{"id":"upx1QJOGiBhr"},"source":["##6.1 하이퍼 파라미터 적용"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4844978,"status":"ok","timestamp":1644466167235,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"z4aYBGuGiGd7","outputId":"4ad6362e-9a6a-40e5-aa00-fa01b923aef6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 5 candidates, totalling 15 fits\n","Logistic Regression best C parameter:  {'C': 10}\n"]}],"source":["# 1시간 20분 소요\n","from sklearn.model_selection import GridSearchCV\n","\n","#최적 C 값 도출 튜닝 수행. CV는 3 폴드 세트로 설정\n","params = { 'C':[0.01, 0.1, 1, 5, 10]}\n","grid_cv_lr = GridSearchCV(lr_clf ,param_grid=params , cv=3 , scoring='accuracy' , verbose=1 )  #verbose : GridSearchCV iteration마다 수행 결과 메시지 출력\n","grid_cv_lr.fit(X_train_tfidf_vect , y_train)\n","print('Logistic Regression best C parameter: ',grid_cv_lr.best_params_ )"]},{"cell_type":"markdown","metadata":{"id":"PODV81MDiOnL"},"source":["##6.2 예측"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":640,"status":"ok","timestamp":1644466532173,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"H-j9LtgLQzD7","outputId":"f0476390-56ec-4079-b759-d6df39c0b22f"},"outputs":[{"name":"stdout","output_type":"stream","text":["TF-IDF Vectorized Logistic Regression의 예측 정확도는 0.701\n"]}],"source":["#최적 C 값으로 학습된 grid_cv로 예측 수행하고 정확도 평가\n","pred = grid_cv_lr.predict(X_test_tfidf_vect)\n","print('TF-IDF Vectorized Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"]},{"cell_type":"markdown","metadata":{"id":"TI7zbU2GRWu8"},"source":["#7. 파이프라인(Pipeline)"]},{"cell_type":"markdown","metadata":{"id":"aP61sAAmRZ6t"},"source":["\u003e 지금까지의 작업 내용을 간략하게 정리하면 다음과 같다.\n","1. 데이터 전처리\n","2. 모델 생성\n","3. 예측\n","\n","\u003e 이러한 일련의 과정들을 하나의 파이프에서 물이 흘러가듯 표현하는 방식이 파이프라인이다."]},{"cell_type":"markdown","metadata":{"id":"08b1A10ZSdEj"},"source":["##7.1 기본 파이프 라인"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":648676,"status":"ok","timestamp":1644467710050,"user":{"displayName":"조윤기","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16540728680765353779"},"user_tz":-540},"id":"eAZ9zDsHSZ8c","outputId":"c2a588b6-d5b7-49bd-f7d4-ae498a291f5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pipeline을 통한 Logistic Regression의 예측 정확도는 0.701\n"]}],"source":["# 11분\n","from sklearn.pipeline import Pipeline\n","\n","#TfidfVectorizer 객체를 tfidf_vect 객체명으로, LogisticRegression객체를 lr_clf 객체명으로 생성하는 Pipeline 생성\n","pipeline = Pipeline([\n","    ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300)),\n","    ('lr_clf', LogisticRegression(max_iter=1000, C=10))\n","])\n","\n","#별도의 TfidfVectorizer객체의 fit_transform( )과 LogisticRegression의 fit(), predict( )가 필요 없음\n","#pipeline의 fit( ) 과 predict( ) 만으로 한꺼번에 Feature Vectorization과 ML 학습/예측이 가능\n","pipeline.fit(X_train, y_train)\n","pred = pipeline.predict(X_test)\n","print('Pipeline을 통한 Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"]},{"cell_type":"markdown","metadata":{"id":"Tc3qaqTYSf8L"},"source":["##7.2 하이퍼파라미터 추가"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"4UrabRuESirT"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 27 candidates, totalling 81 fits\n","{'lr_clf__C': 10, 'tfidf_vect__max_df': 300, 'tfidf_vect__ngram_range': (1, 2)} 0.7508392732054331\n","Pipeline을 통한 Logistic Regression의 예측 정확도는 0.701\n"]}],"source":["# 8시간 정도 진행했지만 모델이 안만들어짐\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","pipeline = Pipeline([\n","    ('tfidf_vect', TfidfVectorizer(stop_words='english')),\n","    ('lr_clf', LogisticRegression(max_iter=1000))\n","])\n","\n","#Pipeline에 기술된 각각의 객체 변수에 언더바(_)2개를 연달아 붙여 GridSearchCV에 사용될 파라미터/하이퍼 파라미터 이름과 값을 설정 \n","params = { 'tfidf_vect__ngram_range': [(1,1), (1,2), (1,3)],\n","           'tfidf_vect__max_df': [100, 300, 700],\n","           'lr_clf__C': [1,5,10]\n","}\n","\n","#GridSearchCV의 생성자에 Estimator가 아닌 Pipeline 객체 입력\n","grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3 , scoring='accuracy',verbose=1)  #verbose : GridSearchCV iteration마다 수행 결과 메시지 출력\n","grid_cv_pipe.fit(X_train , y_train)\n","print(grid_cv_pipe.best_params_ , grid_cv_pipe.best_score_)\n","\n","pred = grid_cv_pipe.predict(X_test)\n","print('Pipeline을 통한 Logistic Regression의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMoPhLvWhfPmsv5BW9dgRxY","collapsed_sections":[],"name":"2. 텍스트 분류 실습","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}